Why car tech companies should look to Amazon for inspiration

The most advanced automotive tech available ahead of the arrival of fully self-driving vehicles should, ideally, be incredibly sophisticated, and completely invisible to users. So far, of all the major tech companies, Amazon is the closest to striking that balance.

Computers are complicated, and one thing drivers don’t need in cars is complication. The safe operation of big, heavy, motor-powered vehicles that can travel as high speed relies heavily on an attentive driver paying attention to the control systems they use to make the car go, as well as to the world around them. Computers, whatever other advantages they have, require attention and focus to operate; figuring out how to make a computer do what you want, especially for any kind of advanced operationn, was always (and remains for many) a learning experience nearly on par with mastering a new language.

You probably shouldn’t be learning a language while operating a 2,600-pound piece of metal going 60 miles-per-hour. Not if you want to do either well, at least.

Computers have come a long way in terms of being intuitive to pick up and use, both thanks to user comfort with computing in general, and improvements on usability and interaction models. The iPhone is probably the single best example of a computing device whose interaction mechanics were expressed in a way that made it generally easy to operate for users of varying levels of interest or expertise in computers overall.

But even the iPhone still requires attention and focus to accomplish basic tasks, and even with its general ease of use, there are subtleties of its interface, features and systems which many everyday users likely still don’t grasp: The lasting appeal of ‘how-to’ guides for even simple features in terms of drawing traffic from search proves this to be true.

The fact is that any visual UI that exists in software is going to take time and attention to learn, and sustained attention to operate. This is true of in-car infotainment systems including OEM-specific platforms, as well as CarPlay and Android Auto. And it’s true still for dash-mounted climate control and driver-assist features, even when these are manipulated via essentially standardized physical controls, like buttons, dials and switches. Plus, with these more basic features, there’s no underlying intelligence – they don’t have any sense of context, and they generally operate in isolation.

Where Amazon seems to have made a leap that would have particular benefit in the automotive world is in making invisible to users the computing layer, while retaining contextual intelligence and computing power behind-the-scenes.

“I think what technology is finally realizing is that technology is subordinate to humans,” Shopify CEO and founder Tobi Lütke explained during a recent talk at Startupfest in Montreal. “We’re starting to be not too enamored anymore with “ooh it’s a computer,” and it being something we need to learn.”

Lütke then cited theDash buttonas the perfect example of what he meant. At first, OEMs rushed to build computers into their appliances, with full touchscreen UIs and plenty of integrations. But, Lütke noted, we didn’t need our washing machines to be a computers with a touchscreen, we just needed a connected button.

Indeed, for cases like the ones the Dash is designed to address, we’re much better served with a single button that translates our immediate need into direct action, using Amazon’s backend services to complete the ordinarily complex tasks of address details, shipping infrastructure and payment handling. And it’s contextually intelligent – it knows enough to discount additional presses that follow quickly on the heels of the first.

Dash is a good starting point for creating tech that supports the driver without adding unnecessary complexity to a task that should be as distraction-free as is possible. To the user, it’s a single dimension of interaction possibility, but behind the scenes, it’s making a lot of complicated things happen in a way that makes the most sense for the context in which it’s used. There’s no mastery for the user to acquire, and yet the results serve their intent near-perfectly.

But a single button or switch will always have limits, and that’s where another Amazon tech can serve as a further guide:Alexa, too, can be a teacher for those working on car tech. The voice-powered assistant does require some learning to use well, but it’s a far cry from the learning curve required by any modern visual operating system. Amazon has done this by design, by keeping Alexa’s interaction models limited, effectively ensuring a user can’t get too turned around in the process of becoming an expert operator.

Alexa eschews any visual interface whatsoever, which is one reason why it’s often perceived by critics as being more successful as a voice-powered assistant than competitors from Apple, Microsoft and Google. There’s no fall-back state for users; they either learn how to operate it by voice or they don’t operate it at all. Pair that with Amazon’s relatively narrow rules for what kind of phrasing Alexa will successfully interpret, and you have a small set of knowledge users have to acquire, with only one means to acquire it. The learning curve might be steeper initially than if they’d tried to do full natural language processing, but it’s also shorter, meaning users can pick it up quicker and be more successful with their queries over the long term.

It’s exactly the kind of interface that makes sense for an in-car computing platform. That steeper, shorter learning curve is like the one involved in learning how to drive a car in the first place: it’s a new set of actions different from nearly anything else we do in life, but it’s tightly circumscribed, and giving users a very finite, but highly distinct set of new things to learn. A voice-based interface would be similar, and would also have the advantage of skipping any kind of visual interface, which, as mentioned above, only adds complexity if presented in tandem with an audio equivalent.

And Alexa can have the same kind of front-end simplicity and back-end sophistication that Dash possesses, albeit with an expanded function set and greater contextual depth. It’s why projects like the joint effort betweenHonda and SoftBank to build a robotic driving assistantthat aims to foster “friendship” might actually have more potential than other pairings between cars and computer companies that focus more on translating existing models of mobile computing to in-vehicle experiences. We need driving companions ahead of full autonomy, not driving computers.

All of the efforts of tech not related to fully automating the driving experience should aim to give drivers more reason to pay attention to what they’re doing. And that’s why Amazon’s efforts to increasingly defray the mental load involved with platform mastery by effectively making the platform invisible have so much potential in transportation tech.

“Until you get to the point where the driver can be completely disengaged, which is this level 4 autonomy, then there shouldn’t be an intermediate step where the driver is less engaged,” Bryson Gardner, CEO and co-founder ofPearl Automationtold me in a recent interview. “The delicate part is, that if you have something that’s really stressful for the driver, than you do want to eliminate that stress.”

Computer proficiency is kind of badge of honor, something that, since the dawn of computing platforms, has been earned through effort, time and mental labor. Even with the advent of more democratizing UI, as on the Android and iOS smartphones we use every day, the bar remains too high for something we intend to use while safely controlling potentially deadly machines. Cars, more than anything, provide the opportunity to explore what comes next for computers, when sophistication is invisible and everyone starts out an expert.